# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qdggnu3C8Jl0lJcei-R7ngJwN7ybpou0
"""

# Using Pandas for dataframe creation, Numpy for array operations, NLTK for producing word cloud
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import re
import string

import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import words
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')
nltk.download('omw-1.4')

from collections import Counter

from matplotlib import pyplot as plt
from matplotlib import ticker
import seaborn as sns
import plotly.express as px

sns.set(style="darkgrid")
from google.colab import drive
drive.mount('/content/drive')

def check_interaction(Interaction, Text, Search_List, Highlight_List, Topic_List, Document_List, Word_List):
  if( Interaction == "Search"):
        Search_List.append(Text)
  elif( Interaction == "Highlight"):
        Highlight_List.append(Text)
  elif( Interaction == "Doc_open"):
        Document_List.append(Text)
  elif( Interaction == "Reading"):
        Word_List.append(Text)
  elif( Interaction == "Topic_change"):
        Topic_List.append(Text.split(" (")[1][:-1])


def counter(L):
  Ans = {}
  for word in L:
    Ans[word] = L.count(word)
  Sorted_Ans = {key: value for key, value in sorted(Ans.items(), key=lambda item: item[1], reverse=True)}
  New_L = list(Sorted_Ans.items())
  Result = np.array(New_L)
  return Result


def frequent_words(Para):
  L = Para.split(" ")
  Ans = {}
  for word in L:
    Ans[word] = L.count(word)
  Sorted_Ans = {key: value for key, value in sorted(Ans.items(), key=lambda item: item[1], reverse=True)}
  New_L = list(Sorted_Ans.items())
  Result = np.array(New_L)
  return Result[:30]

from threading import current_thread
dataset = 1
user = 1
type = ['Arms', 'Terrorist','Disappearance']
dataset_name = ['Armsdealing', 'TerroristActivity', 'Disappearance']
while dataset < 4:
  user = 1
  doc_type = type[dataset-1]
  curr_dataset = dataset_name[dataset-1]
  while user < 9:
    Interactions = pd.read_json("/content/drive/MyDrive/ProvSegments/Dataset_{dataset_num}/User_Interactions/{doc}_P{user_num}_InteractionsLogs.json".format(dataset_num=dataset, doc=doc_type, user_num=user))
    Segments = pd.read_csv('/content/drive/MyDrive/ProvSegments/Dataset_{dataset_num}/Segmentation/{doc}_P{user_num}_20_4_6_Prov_Segments.csv'.format(dataset_num = dataset, doc=doc_type, user_num = user))
    Document_Directory = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_{dataset_num}/Documents/Documents_Dataset_{dataset_num}.json'.format(dataset_num = dataset))

    Log_Data = pd.DataFrame(columns=['Start Time', 'End Time', 'Duration', 'Number of Searches', 'Searched Keywords', 'Number of Highlights', 'Highlighted Keywords', 'Number of Topic Changes', 'Topic Change Reason', 'Number of Documents Opened', 'Document Name', 'Number of Documents Read', 'Frequent Words'])
    Interaction_Type = Interactions['InteractionType']
    Text = Interactions['Text']
    Durations = Interactions['duration']
    Time = Interactions['time']
    End_Time = Segments['end']*10
    Start_Time = Segments['start']*10
    Duration = Segments['length (sec)']*10
    Document_Content = Document_Directory['contents']


    i = 0
    j = 0

    while i < len(End_Time):
      Search = []
      Highlight = []
      Documents = []
      Reading = []
      Topic = []
      # 50594
      while j < len(Interactions) and Time[j]  <= int(End_Time.iloc[-1]):
        if Time[j] == int(End_Time[i]):
          check_interaction(Interaction_Type[j], Text[j], Search, Highlight, Topic, Documents, Reading)

          Log_Data.at[i,'Start Time'] = Start_Time[i]/10
          Log_Data.at[i,'End Time'] = End_Time[i]/10
          Log_Data.at[i,'Duration'] = Duration[i]/10

          Log_Data.at[i,'Number of Searches'] = len(Search)
          Log_Data.at[i,'Searched Keywords'] = counter(Search)

          Log_Data.at[i,'Number of Highlights'] = len(Highlight)
          Log_Data.at[i,'Highlighted Keywords'] = counter(Highlight)

          Log_Data.at[i,'Number of Topic Changes'] = len(Topic)
          Log_Data.at[i,'Topic Change Reason'] = Topic

          Log_Data.at[i,'Number of Documents Opened'] = len(Documents)
          Log_Data.at[i,'Document Name'] = counter(Documents)

          Log_Data.at[i,'Number of Documents Read'] = len(Reading)
          Log_Data.at[i,'Frequent Words'] = counter(Reading)

          Search = []
          Highlight = []
          Documents = []
          Reading = []
          Topic = []

          i += 1
          j += 1

        else:
          check_interaction(Interaction_Type[j], Text[j], Search, Highlight, Topic, Documents, Reading)
          j += 1

    remove_user = lambda x: re.sub(r'@[\w]*','',str(x))
    Document_Content = Document_Content.apply(remove_user)

    to_lower = lambda x:x.lower()
    Document_Content = Document_Content.apply(to_lower)

    Document_Content = Document_Content.str.replace("(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "")
    more_words= ['br', 'brbr','report','date','per','year','month','years','months','mm', 'mr', 'make', '%brbr%']
    stop_words=set(stopwords.words('english'))
    stop_words.update(more_words)

    remove_words = lambda x: ' '.join([word for word in x.split() if word not in stop_words])
    Document_Content = Document_Content.apply(remove_words)

    stemmer = PorterStemmer()
    Stem = lambda x:  ''.join([stemmer.stem(y) for y in x])
    Document_Content = Document_Content.apply(Stem)

    lemmatizer = WordNetLemmatizer()
    Lem = lambda x: ''.join([lemmatizer.lemmatize(w) for w in x])
    Document_Content = Document_Content.apply(Lem)

    FW = Log_Data['Frequent Words']
    Complete_word_cloud = []
    for i in range(len(FW)):
      x = FW[i]
      for j in range(len(x)):
        if FW[i][j][0] not in Complete_word_cloud:
          Complete_word_cloud.append(FW[i][j][0])
    Complete_word_cloud = pd.DataFrame(Complete_word_cloud).iloc[: ,0]
    EW = Complete_word_cloud
    Content = ''

    for i in range(len(EW)):
      Index = ((int(EW[i].split(" ")[1]))-1)
      Content += Document_Content[Index]
    Complete_Words = frequent_words(Content)


    for i in range(len(FW)):
      Content = ''
      for j in range(len(FW[i])):
        Index = int(FW[i][j][0].split(" ")[1])-1
        Content += Document_Content[Index]
      Words = frequent_words(Content)
      Log_Data.at[i,'Frequent Words'] = Words


    Segment_Number = []
    Words = []
    Frequency = []
    for i in range(len(FW)):
        for j in range(len(FW[i])):
          Segment_Number.append("Segment %d"%i)
          if FW[i][j][0] == '':
            Words.append('No Keywords')
          else:
              Words.append(FW[i][j][0])
          Frequency.append(int(FW[i][j][1]))

    WC_1 = ({'Dataset': '{dataset}'.format(dataset = curr_dataset), 'User Number': 'User{user_num}'.format(user_num =user), 'Segment Number': Segment_Number, 'Word' : Words, 'Frequency': Frequency})
    Words_Count = pd.DataFrame(WC_1)
    Words_Count.to_excel(r'/content/drive/MyDrive/ProvSegments/Word_Count/Dataset-{dataset_num}/User-{user_num}.xlsx'.format(dataset_num = dataset, user_num = user))


    Doc = Log_Data['Document Name']
    Segment_Number = []
    Document = []
    Frequency = []
    for i in range(len(Doc)):
      for j in range(len(Doc[i])):
        Segment_Number.append("Segment %d"%i)
        Document.append(Doc[i][j][0])
        Frequency.append(int(Doc[i][j][1]))

    DC_1 = ({'Dataset': '{dataset}'.format(dataset = curr_dataset), 'User Number': 'User{user_num}'.format(user_num =user), 'Segment Number': Segment_Number, 'Document Name' : Document, 'Frequency': Frequency})
    Doc_Count = pd.DataFrame(DC_1)
    Doc_Count.to_excel(r'/content/drive/MyDrive/ProvSegments/Doc_Count/Dataset-{dataset_num}/User-{user_num}.xlsx'.format(dataset_num = dataset, user_num = user))


    Log_Data = pd.DataFrame(Log_Data)
    Log_Data.to_excel(r'/content/drive/MyDrive/ProvSegments/Front-end/Dataset-{dataset_num}/User-{user_num}.xlsx'.format(dataset_num = dataset, user_num = user))
    print("Log_Data for user: {user_num} in dataset: {dataset_num}.".format(dataset_num = dataset, user_num = user))
    user += 1
  dataset += 1