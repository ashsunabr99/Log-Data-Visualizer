# -*- coding: utf-8 -*-
"""Information_Visualisation_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CyVZl5neL66WcAZ4vfS2FzY85ECQIvrm

# Importing all the necessary libraries
"""

# Using Pandas for dataframe creation, Numpy for array operations, NLTK for producing word cloud
import pandas as pd 
import numpy as np 

import matplotlib.pyplot as plt 
import re
import string

import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import words
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')
nltk.download('omw-1.4')

from collections import Counter

from matplotlib import pyplot as plt
from matplotlib import ticker
import seaborn as sns
import plotly.express as px

sns.set(style="darkgrid")
from google.colab import drive
drive.mount('/content/drive')

"""# User 1, Dataset1 -> ArmsDealing"""

user = 1
Interactions = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_1/User_Interactions/Arms_P%d_InteractionsLogs.json'%user)
Segments = pd.read_csv("/content/drive/MyDrive/ProvSegments/Dataset_1/Segmentation/Arms_P%d_20_4_6_Prov_Segments.csv"%user)
Document_Directory = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_1/Documents/Documents_Dataset_1.json')

"""# User 1, Dataset2 -> TerroristActivity"""

Interactions = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_2/User Interactions/Terrorist_P8_InteractionsLogs.json')
Segments = pd.read_csv('/content/drive/MyDrive/ProvSegments/Dataset_2/Segmentation/Terrorist_P8_20_4_6_Prov_Segments.csv')
Document_Directory = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_2/Documents/Documents_Dataset_2.json')

"""# User 1, Dataset3 -> Disappearance"""

Interactions = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_3/User Interactions/Disappearance_P8_InteractionsLogs.json')
Segments = pd.read_csv('/content/drive/MyDrive/ProvSegments/Dataset_3/Segmentation/Disappearance_P8_20_3_6_Prov_Segments.csv')
Document_Directory = pd.read_json('/content/drive/MyDrive/ProvSegments/Dataset_3/Document/Documents_Dataset_3.json')

"""# Getting basic information about the datasets"""

Interactions.info()

Segments.info()

Document_Directory.info()

"""# Deriving selected colums from the dataframe for Log_Data for all segments"""

Log_Data = pd.DataFrame(columns=['Start Time', 'End Time', 'Duration', 'Number of Searches', 'Searched Keywords', 'Number of Highlights', 'Highlighted Keywords', 'Number of Topic Changes', 'Topic Change Reason', 'Number of Documents Opened', 'Document Name', 'Number of Documents Read', 'Frequent Words'])
Interaction_Type = Interactions['InteractionType']
Text = Interactions['Text']
Durations = Interactions['duration']
Time = Interactions['time']
End_Time = Segments['end']*10
Start_Time = Segments['start']*10
Duration = Segments['length (sec)']*10

"""# Function to check type of interaction and populate the appropriate lists corresponding to it"""

def check_interaction(Interaction, Text, Search_List, Highlight_List, Topic_List, Document_List, Word_List):
  if( Interaction == "Search"):
        Search_List.append(Text)
  elif( Interaction == "Highlight"):
        Highlight_List.append(Text)
  elif( Interaction == "Doc_open"):
        Document_List.append(Text)
  elif( Interaction == "Reading"):
        Word_List.append(Text)
  elif( Interaction == "Topic_change"):
        Topic_List.append(Text.split(" (")[1][:-1])

"""# Function to determine frequency of words highlighted, words searched, number of documents opened, number of documents read and number of topic changes"""

def counter(L):
  Ans = {}
  for word in L:
    Ans[word] = L.count(word)
  Sorted_Ans = {key: value for key, value in sorted(Ans.items(), key=lambda item: item[1], reverse=True)}
  New_L = list(Sorted_Ans.items())
  Result = np.array(New_L)
  return Result

"""# Populating Log data for a user corresponding to each segment"""

i = 0 
j = 0

while i < len(End_Time):

  Search = []
  Highlight = []
  Documents = []
  Reading = []
  Topic = []
  # 50594
  while j < len(Interactions) and Time[j]  <= 50594:

    End = int(End_Time[i])
    if Time[j] == End:
      check_interaction(Interaction_Type[j], Text[j], Search, Highlight, Topic, Documents, Reading)

      Log_Data.at[i,'Start Time'] = Start_Time[i]/10
      Log_Data.at[i,'End Time'] = End_Time[i]/10
      Log_Data.at[i,'Duration'] = Duration[i]/10

      Log_Data.at[i,'Number of Searches'] = len(Search)
      Log_Data.at[i,'Searched Keywords'] = counter(Search)

      Log_Data.at[i,'Number of Highlights'] = len(Highlight)
      Log_Data.at[i,'Highlighted Keywords'] = counter(Highlight)

      Log_Data.at[i,'Number of Topic Changes'] = len(Topic)
      Log_Data.at[i,'Topic Change Reason'] = Topic

      Log_Data.at[i,'Number of Documents Opened'] = len(Documents)
      Log_Data.at[i,'Document Name'] = counter(Documents)

      Log_Data.at[i,'Number of Documents Read'] = len(Reading)
      Log_Data.at[i,'Frequent Words'] = counter(Reading)

      Search = []
      Highlight = []
      Documents = []
      Reading = []
      Topic = []

      i += 1
      j += 1

    else:
      check_interaction(Interaction_Type[j], Text[j], Search, Highlight, Topic, Documents, Reading)
      j += 1

"""Display the Log_data Dataframe"""

Log_Data

"""# Cleaning the document content"""

Document_Content = Document_Directory['contents']

remove_user = lambda x: re.sub(r'@[\w]*','',str(x))
Document_Content = Document_Content.apply(remove_user)

to_lower = lambda x:x.lower()
Document_Content = Document_Content.apply(to_lower)

Document_Content = Document_Content.str.replace("(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "")

more_words= ['br', 'brbr','report','date','per','year','month','years','months','mm', 'mr', 'make', '%brbr%']
stop_words=set(stopwords.words('english'))
stop_words.update(more_words)

remove_words = lambda x: ' '.join([word for word in x.split() if word not in stop_words])
Document_Content = Document_Content.apply(remove_words)

stemmer = PorterStemmer()
Stem = lambda x:  ''.join([stemmer.stem(y) for y in x])
Document_Content = Document_Content.apply(Stem)

lemmatizer = WordNetLemmatizer()
Lem = lambda x: ''.join([lemmatizer.lemmatize(w) for w in x])
Document_Content = Document_Content.apply(Lem)

"""# Function to determine top 30 frequent words in our paragraph"""

def frequent_words(Para):
  L = Para.split(" ")
  Ans = {}
  for word in L:
    Ans[word] = L.count(word)
  Sorted_Ans = {key: value for key, value in sorted(Ans.items(), key=lambda item: item[1], reverse=True)}
  New_L = list(Sorted_Ans.items())
  Result = np.array(New_L)
  return Result[:30]

"""# Extracting 30 most frequently occuring documents from all the read documents in entire Log Data"""

Complete = Log_Data['Frequent Words']
Complete_word_cloud = []
for i in range(len(Complete)):
  x = Complete[i]
  for j in range(len(x)):
    if Complete[i][j][0] not in Complete_word_cloud:
      Complete_word_cloud.append(Complete[i][j][0])

Complete_word_cloud = pd.DataFrame(Complete_word_cloud).iloc[: ,0]

EW = Complete_word_cloud
Content = ''

for i in range(len(EW)):
  Index = ((int(EW[i].split(" ")[1]))-1)
  Content += Document_Content[Index]

Complete_Words = frequent_words(Content)
Complete_word_cloud

"""# Extracting 30 most frequently occuring words from all the read documents per segment"""

FW = Log_Data['Frequent Words']

for i in range(len(FW)):
  Content = ''
  for j in range(len(FW[i])):
    Index = int(FW[i][j][0].split(" ")[1])-1
    Content += Document_Content[Index]    
  Words = frequent_words(Content)
  Log_Data.at[i,'Frequent Words'] = Words

"""Display the Log_data Dataframe

"""

Log_Data

FWC = Log_Data['Frequent Words']
Segment_Number = []
Words = [] 
Frequency = []
for i in range(len(FWC)):
    for j in range(len(FWC[i])):
     Segment_Number.append("Segment %d"%i)
     if FWC[i][j][0] == '':
       Words.append('No Keywords')
     else:
        Words.append(FWC[i][j][0])
     Frequency.append(int(FWC[i][j][1]))

WC_1 = ({'User Number': 'User1', 'Segment Number': Segment_Number, 'Word' : Words, 'Frequency': Frequency})
Words_Count = pd.DataFrame(WC_1)

Words_Count.to_excel(r'/content/drive/MyDrive/ProvSegments/Word_Count/Dataset-1/User-%d.xlsx'%user)

"""# Function to get words for generation of a word cloud from all the documents in a segment by combining them and forming a paragraph"""

def get_word(FW):
  text = []
  for i in range(len(FW)):
    text.append(FW[i][0])
  return ' '.join(text)

"""# Making word cloud for 30 most frequently occuring words from all the read documents in entire log data"""

from wordcloud import WordCloud
words = get_word(Complete_Words)
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, include_numbers=True).generate(words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""# Making word cloud for 30 most frequently occuring words from all the read documents per segment"""

from wordcloud import WordCloud
for i in range(len(Log_Data['Frequent Words'])):
  words = get_word(Log_Data['Frequent Words'][i])
  if (len(Log_Data['Frequent Words'][i])) > 1:
    print("\nSegment %d" %i)
    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110,include_numbers=True).generate(words)
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    #plt.show()
    fig1 = plt.gcf()
    fig1.savefig('/content/drive/MyDrive/ProvSegments/WordCloud/ArmsDealing/P2/WC%d.png'%i,dpi=100)
    plt.show()

"""# Exporting the dataframe to JSON"""

Log_Data = pd.DataFrame(Log_Data)

Log_Data.to_excel(r'/content/drive/MyDrive/ProvSegments/Front-end/Dataset-1/User-8_3.xlsx')



